import { create } from 'zustand';
import type { QueryClient } from '@tanstack/react-query';
import {
  getRecommendedUploadConfig,
  type UploadBatchProgress,
  type UploadFileProgress,
} from '../types/upload';
import { mediaFileService } from '../services/mediaFileService';
import type { ProcessedAudioFile } from '../services/audioFileProcessor';
import { supabase } from '../services/supabase';

export interface R2UploadState {
  // Upload state
  currentBatch: UploadBatchProgress | null;
  isUploading: boolean;
  showProgressToast: boolean;

  // Callbacks
  onUploadComplete?: (completedFiles: string[], failedFiles: string[]) => void;
  onBatchComplete?: (batchProgress: UploadBatchProgress) => void;

  // Actions
  startUpload: (
    files: ProcessedAudioFile[],
    projectData: {
      languageEntityId: string;
      languageEntityName: string;
      audioVersionId: string;
    },
    userId: string,
    queryClient?: QueryClient, // QueryClient instance for table refreshes
    projectId?: string // Project ID for targeted query invalidation
  ) => Promise<void>;

  cancelUpload: () => void;
  closeProgressToast: () => void;
  setOnUploadComplete: (
    callback?: (completedFiles: string[], failedFiles: string[]) => void
  ) => void;
  setOnBatchComplete: (
    callback?: (batchProgress: UploadBatchProgress) => void
  ) => void;

  // Internal actions
  updateBatchProgress: (progress: UploadBatchProgress) => void;
  updateFileProgress: (progress: UploadFileProgress) => void;
  resetUploadState: () => void;
}

export const useR2UploadStore = create<R2UploadState>((set, get) => ({
  // Initial state
  currentBatch: null,
  isUploading: false,
  showProgressToast: false,
  onUploadComplete: undefined,
  onBatchComplete: undefined,

  startUpload: async (files, projectData, userId, queryClient, projectId) => {
    const state = get();

    if (state.isUploading) {
      throw new Error('Another upload is already in progress');
    }

    console.log('üöÄ Starting R2 by-id upload for', files.length, 'files');

    // Validate all files have required selections
    const invalidFiles = files.filter(
      f =>
        !f.selectedBookId ||
        !f.selectedChapterId ||
        !f.selectedStartVerseId ||
        !f.selectedEndVerseId
    );

    if (invalidFiles.length > 0) {
      throw new Error(
        `${invalidFiles.length} files are missing book/chapter/verse selections`
      );
    }

    // Prepare metadata for R2 upload
    const uploadMetadata = {
      language: projectData.languageEntityName,
    };

    // Set initial upload state
    set({
      isUploading: true,
      showProgressToast: true,
      currentBatch: null,
    });

    try {
      // Get metadata for each file to include in R2 upload (parallelized with Promise.all)
      console.log('üìä Fetching metadata for', files.length, 'files...');
      const metadataStartTime = Date.now();

      const filesWithMetadata = await Promise.all(
        files.map(async (file, index) => {
          try {
            // Get book OSIS, chapter number, and verse numbers in parallel
            const [bookOsis, chapterNumber, verseNumbers] = await Promise.all([
              mediaFileService.getBookOsisFromChapter(file.selectedChapterId!),
              mediaFileService.getChapterNumber(file.selectedChapterId!),
              mediaFileService.getVerseNumbers([
                file.selectedStartVerseId!,
                file.selectedEndVerseId!,
              ]),
            ]);

            const startVerse = verseNumbers[file.selectedStartVerseId!];
            const endVerse = verseNumbers[file.selectedEndVerseId!];

            return {
              file,
              metadata: {
                ...uploadMetadata,
                book: bookOsis,
                chapter: chapterNumber.toString(),
                startverse: startVerse.toString(),
                endverse: endVerse.toString(),
              },
            };
          } catch (error) {
            console.warn(
              `‚ö†Ô∏è Failed to get metadata for file ${index + 1}/${files.length} (${file.file.name}), using fallback:`,
              error
            );
            // Use filename parsed data as fallback
            return {
              file,
              metadata: {
                ...uploadMetadata,
                book: file.filenameParseResult.detectedBook || 'unknown',
                chapter: (
                  file.filenameParseResult.detectedChapter || 0
                ).toString(),
                startverse: (
                  file.filenameParseResult.detectedStartVerse || 0
                ).toString(),
                endverse: (
                  file.filenameParseResult.detectedEndVerse ||
                  file.filenameParseResult.detectedStartVerse ||
                  0
                ).toString(),
              },
            };
          }
        })
      );

      const metadataTime = Date.now() - metadataStartTime;
      console.log(
        `‚úÖ Metadata fetched in ${metadataTime}ms (${(metadataTime / files.length).toFixed(1)}ms per file)`
      );

      // Batch create pending media_files rows (much faster than individual inserts)
      console.log(
        'üìù Creating pending media files batch for',
        filesWithMetadata.length,
        'files'
      );
      const startTime = Date.now();

      const pendingIds = await mediaFileService.createPendingMediaFilesBatch({
        processedFiles: filesWithMetadata.map(item => item.file),
        projectData,
        userId,
      });

      const batchTime = Date.now() - startTime;
      console.log(
        `‚úÖ Created ${pendingIds.length} pending files in ${batchTime}ms (${(batchTime / pendingIds.length).toFixed(1)}ms per file)`
      );
      console.log('üìã Pending media file IDs:', pendingIds);

      // Get by-id presigned PUT URLs with chunking for large batches
      console.log(
        'üîó Requesting upload URLs for',
        pendingIds.length,
        'files...'
      );
      const urlStartTime = Date.now();

      // Pass original filenames mapping for backend object key generation
      const originalFilenames: Record<string, string> = {};
      filesWithMetadata.forEach((item, index) => {
        originalFilenames[pendingIds[index]] = item.file.file.name;
      });

      // For large batches, chunk the URL requests to avoid edge function timeouts
      const urlChunkSize = 25; // Smaller chunks for URL generation
      let allUrlResponses: Array<{
        id: string;
        objectKey: string;
        uploadUrl: string;
      }> = [];

      if (pendingIds.length <= urlChunkSize) {
        // Single request for small batches
        const requestBody = {
          mediaFileIds: pendingIds,
          expirationHours: 24,
          originalFilenames,
        };

        const { data, error } = await supabase.functions.invoke(
          'get-upload-urls-by-id',
          {
            body: requestBody,
          }
        );

        if (error) {
          console.error('‚ùå Edge function error:', error);
          throw new Error(`get-upload-urls-by-id failed: ${error.message}`);
        }

        const functionResponse = data?.data;
        const byId = functionResponse as {
          success: boolean;
          media?: Array<{ id: string; objectKey: string; uploadUrl: string }>;
          errors?: Record<string, string>;
        };

        if (!byId.success || !byId.media) {
          const errorDetails = Object.entries(byId.errors || {})
            .map(([id, error]) => `${id}: ${error}`)
            .join('; ');
          throw new Error(`Upload URL generation failed: ${errorDetails}`);
        }

        allUrlResponses = byId.media;
      } else {
        // Chunked URL requests for large batches
        console.log(
          `üìä Using chunked URL requests: ${Math.ceil(pendingIds.length / urlChunkSize)} chunks of ${urlChunkSize} IDs`
        );

        for (let i = 0; i < pendingIds.length; i += urlChunkSize) {
          const chunk = pendingIds.slice(i, i + urlChunkSize);
          const chunkNum = Math.floor(i / urlChunkSize) + 1;
          const totalChunks = Math.ceil(pendingIds.length / urlChunkSize);

          console.log(
            `üîó Requesting URLs for chunk ${chunkNum}/${totalChunks} (${chunk.length} files)`
          );
          const chunkStartTime = Date.now();

          // Create chunk-specific filename mapping
          const chunkFilenames: Record<string, string> = {};
          chunk.forEach(id => {
            if (originalFilenames[id]) {
              chunkFilenames[id] = originalFilenames[id];
            }
          });

          const requestBody = {
            mediaFileIds: chunk,
            expirationHours: 24,
            originalFilenames: chunkFilenames,
          };

          const { data, error } = await supabase.functions.invoke(
            'get-upload-urls-by-id',
            {
              body: requestBody,
            }
          );

          if (error) {
            console.error(
              `‚ùå Edge function error for chunk ${chunkNum}:`,
              error
            );
            throw new Error(
              `get-upload-urls-by-id failed for chunk ${chunkNum}: ${error.message}`
            );
          }

          const functionResponse = data?.data;
          const byId = functionResponse as {
            success: boolean;
            media?: Array<{ id: string; objectKey: string; uploadUrl: string }>;
            errors?: Record<string, string>;
          };

          if (
            !byId.success ||
            !byId.media ||
            byId.media.length !== chunk.length
          ) {
            const errorDetails = Object.entries(byId.errors || {})
              .map(([id, error]) => `${id}: ${error}`)
              .join('; ');
            throw new Error(
              `Upload URL generation failed for chunk ${chunkNum}: ${errorDetails}`
            );
          }

          allUrlResponses.push(...byId.media);

          const chunkTime = Date.now() - chunkStartTime;
          console.log(
            `‚úÖ Chunk ${chunkNum} URLs generated in ${chunkTime}ms (${(chunkTime / chunk.length).toFixed(1)}ms per URL)`
          );
        }
      }

      const urlTime = Date.now() - urlStartTime;
      console.log(
        `‚úÖ All upload URLs generated in ${urlTime}ms (${(urlTime / pendingIds.length).toFixed(1)}ms per URL)`
      );

      // Validate that we have URLs for all pending IDs
      if (allUrlResponses.length !== pendingIds.length) {
        const missingIds = pendingIds.filter(
          id => !allUrlResponses.some(m => m.id === id)
        );
        console.error('‚ùå Missing upload URLs for IDs:', missingIds);
        throw new Error(
          `Failed to get upload URLs for ${missingIds.length} files: ${missingIds.join(', ')}`
        );
      }

      const idToUpload = new Map<string, { uploadUrl: string }>();
      allUrlResponses.forEach(m =>
        idToUpload.set(m.id, { uploadUrl: m.uploadUrl })
      );

      // Initialize batch progress and run uploads with optimized concurrency
      const totalSizeMB =
        filesWithMetadata.reduce((sum, f) => sum + f.file.file.size, 0) /
        (1024 * 1024);
      const recommendedConfig = getRecommendedUploadConfig(
        files.length,
        totalSizeMB
      );
      const batchId = crypto.randomUUID();
      const batchProgress: UploadBatchProgress = {
        batchId,
        totalFiles: files.length,
        completedFiles: 0,
        failedFiles: 0,
        files: files.map(f => ({
          fileName: f.file.name,
          fileSize: f.file.size,
          uploadedBytes: 0,
          status: 'pending',
        })),
      };
      get().updateBatchProgress(batchProgress);

      const concurrency = recommendedConfig.concurrency || 3;
      console.log(
        `üöÄ Starting ${files.length} uploads with ${concurrency} concurrent workers (${totalSizeMB.toFixed(1)}MB total)`
      );
      const uploadStartTime = Date.now();

      // Track completed uploads for batch finalization
      const completedUploads: Array<{
        mediaFileId: string;
        fileSize: number;
        durationSeconds: number;
      }> = [];

      // Throttle progress updates to prevent React infinite loops
      const progressThrottleMap = new Map<string, number>(); // fileName -> lastUpdateTime
      const PROGRESS_THROTTLE_MS = 200; // Update at most every 200ms per file
      console.log(
        `üö´ Progress throttling enabled: max ${1000 / PROGRESS_THROTTLE_MS} updates/sec per file`
      );

      let idx = 0;
      const worker = async () => {
        while (idx < filesWithMetadata.length) {
          const current = idx++;
          const item = filesWithMetadata[current];
          const id = pendingIds[current];
          const put = idToUpload.get(id)!;
          try {
            // Initialize file progress as uploading (with throttle tracking)
            const fileName = item.file.file.name;
            const initialProgress: UploadFileProgress = {
              fileName: fileName,
              fileSize: item.file.file.size,
              uploadedBytes: 0,
              status: 'uploading',
            };
            get().updateFileProgress(initialProgress);
            progressThrottleMap.set(fileName, Date.now()); // Initialize throttle tracking

            // Use XMLHttpRequest for progress tracking with timeout
            await new Promise<void>((resolve, reject) => {
              const xhr = new XMLHttpRequest();

              // Set reasonable timeout (5 minutes + extra time for large files)
              const timeoutMs = Math.max(
                300000,
                (item.file.file.size / (1024 * 1024)) * 30000
              ); // 30s per MB
              xhr.timeout = timeoutMs;

              // Track upload progress with throttling to prevent React infinite loops
              xhr.upload.onprogress = e => {
                if (e.lengthComputable) {
                  const now = Date.now();
                  const fileName = item.file.file.name;
                  const lastUpdate = progressThrottleMap.get(fileName) || 0;

                  // Only update if enough time has passed since last update for this file
                  // OR if upload is complete (100%)
                  const isComplete = e.loaded >= e.total;
                  const shouldUpdate =
                    now - lastUpdate >= PROGRESS_THROTTLE_MS || isComplete;

                  if (shouldUpdate) {
                    const progressUpdate: UploadFileProgress = {
                      fileName: fileName,
                      fileSize: e.total,
                      uploadedBytes: e.loaded,
                      status: 'uploading',
                    };
                    get().updateFileProgress(progressUpdate);
                    progressThrottleMap.set(fileName, now);
                  }
                }
              };

              // Handle completion
              xhr.onload = () => {
                if (xhr.status >= 200 && xhr.status < 300) {
                  resolve();
                } else {
                  reject(
                    new Error(`R2 PUT failed: ${xhr.status} ${xhr.statusText}`)
                  );
                }
              };

              // Handle errors
              xhr.onerror = () =>
                reject(new Error('Network error during upload'));
              xhr.onabort = () => reject(new Error('Upload aborted'));
              xhr.ontimeout = () =>
                reject(new Error(`Upload timeout after ${timeoutMs / 1000}s`));

              // Start the upload
              xhr.open('PUT', put.uploadUrl);
              xhr.setRequestHeader('Content-Type', item.file.file.type);
              xhr.send(item.file.file);
            });

            // Track successful upload for batch finalization
            completedUploads.push({
              mediaFileId: id,
              fileSize: item.file.file.size,
              durationSeconds: Math.round(item.file.duration),
            });

            const fp: UploadFileProgress = {
              fileName: item.file.file.name,
              fileSize: item.file.file.size,
              uploadedBytes: item.file.file.size,
              status: 'completed',
            };
            get().updateFileProgress(fp);
            // Clear throttle tracking for completed files
            progressThrottleMap.delete(item.file.file.name);

            // Update the batch progress files array with the completed file
            const fileIndex = batchProgress.files.findIndex(
              f => f.fileName === item.file.file.name
            );
            if (fileIndex !== -1) {
              batchProgress.files[fileIndex] = fp;
            }
            batchProgress.completedFiles++;
            get().updateBatchProgress(batchProgress);
          } catch (e) {
            const fp: UploadFileProgress = {
              fileName: item.file.file.name,
              fileSize: item.file.file.size,
              uploadedBytes: 0,
              status: 'failed',
              error: e instanceof Error ? e.message : 'Upload failed',
            };
            get().updateFileProgress(fp);
            // Clear throttle tracking for failed files
            progressThrottleMap.delete(item.file.file.name);

            // Update the batch progress files array with the failed file
            const fileIndex = batchProgress.files.findIndex(
              f => f.fileName === item.file.file.name
            );
            if (fileIndex !== -1) {
              batchProgress.files[fileIndex] = fp;
            }
            batchProgress.failedFiles++;
            get().updateBatchProgress(batchProgress);
          }
        }
      };

      await Promise.all(
        Array.from(
          { length: Math.min(concurrency, filesWithMetadata.length) },
          () => worker()
        )
      );

      const uploadTime = Date.now() - uploadStartTime;
      const avgTimePerFile = uploadTime / files.length;
      const totalMBPerSecond = (totalSizeMB / (uploadTime / 1000)).toFixed(2);

      console.log(
        `‚úÖ Upload phase completed in ${uploadTime}ms (${avgTimePerFile.toFixed(1)}ms per file, ${totalMBPerSecond}MB/s)`
      );

      // Batch finalize all successful uploads
      if (completedUploads.length > 0) {
        console.log(
          `üìù Finalizing ${completedUploads.length} successful uploads...`
        );
        const finalizeStartTime = Date.now();

        try {
          await mediaFileService.finalizeMediaFilesBatch({
            updates: completedUploads,
          });
          const finalizeTime = Date.now() - finalizeStartTime;
          console.log(
            `‚úÖ Finalized ${completedUploads.length} records in ${finalizeTime}ms`
          );
        } catch (error) {
          console.error(
            '‚ùå Batch finalization failed, falling back to individual updates:',
            error
          );
          // Fallback to individual finalization
          for (const update of completedUploads) {
            try {
              await mediaFileService.finalizeMediaFile(update);
            } catch (individualError) {
              console.error(
                `Failed to finalize ${update.mediaFileId}:`,
                individualError
              );
            }
          }
        }
      }

      // Invalidate queries for completed uploads
      if (queryClient && projectId && completedUploads.length > 0) {
        queryClient.invalidateQueries({
          queryKey: ['media_files_by_project_paginated', projectId],
        });
        queryClient.invalidateQueries({
          queryKey: ['media_files_with_verse_info', projectId],
        });
      }

      const totalTime = Date.now() - metadataStartTime;
      console.log(
        `üéâ Total upload process completed in ${totalTime}ms for ${files.length} files (${batchProgress.completedFiles} successful, ${batchProgress.failedFiles} failed)`
      );

      // Call completion callbacks
      const { onUploadComplete, onBatchComplete } = get();

      const failedFileNames = batchProgress.files
        .filter(f => f.status === 'failed')
        .map(f => f.fileName);

      const completedFileNames = batchProgress.files
        .filter(f => f.status === 'completed')
        .map(f => f.fileName);

      onUploadComplete?.(completedFileNames, failedFileNames);
      onBatchComplete?.(batchProgress);

      // Update final state
      set({
        isUploading: false,
        currentBatch: batchProgress,
      });
    } catch (error) {
      console.error('‚ùå Upload failed:', error);

      // Update state on error
      set({
        isUploading: false,
        currentBatch: null,
      });

      throw error;
    }
  },

  cancelUpload: () => {
    // For the by-id R2 flow, we don't keep abort controllers; noop for now.
    set({
      isUploading: false,
      currentBatch: null,
      showProgressToast: false,
    });
  },

  closeProgressToast: () => {
    set({ showProgressToast: false });
  },

  setOnUploadComplete: callback => {
    set({ onUploadComplete: callback });
  },

  setOnBatchComplete: callback => {
    set({ onBatchComplete: callback });
  },

  updateBatchProgress: progress => {
    set({ currentBatch: progress });
  },

  updateFileProgress: progress => {
    const { currentBatch } = get();
    if (!currentBatch) return;

    // Find the file to update
    const fileIndex = currentBatch.files.findIndex(
      f => f.fileName === progress.fileName
    );
    if (fileIndex === -1) return;

    const currentFile = currentBatch.files[fileIndex];

    // Skip update if no meaningful change (prevents unnecessary re-renders)
    if (
      currentFile.status === progress.status &&
      currentFile.uploadedBytes === progress.uploadedBytes &&
      currentFile.fileSize === progress.fileSize
    ) {
      return;
    }

    // Create optimized update (only update the changed file)
    const updatedFiles = [...currentBatch.files];
    updatedFiles[fileIndex] = progress;

    const updatedBatch: UploadBatchProgress = {
      ...currentBatch,
      files: updatedFiles,
    };

    set({ currentBatch: updatedBatch });
  },

  resetUploadState: () => {
    set({
      currentBatch: null,
      isUploading: false,
      showProgressToast: false,
      onUploadComplete: undefined,
      onBatchComplete: undefined,
    });
  },
}));

// Export aliases for backward compatibility
export const useB2UploadStore = useR2UploadStore;
export const useUploadStore = useR2UploadStore;

// Export upload warning hook (simple implementation)
export const useUploadWarning = () => {
  return {
    showWarning: false,
    warningMessage: '',
    clearWarning: () => {},
  };
};
